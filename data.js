export default [
  {
    title:
      'Transformer models have achieved state-of-the-art results across adiverse range of domains. However, concern over the cost of training the attention mechanism to learn complex dependencies between distant inputs continues to grow. In response, solutions that exploit the structure and sparsity of the learned attention matrix have blossomed. However, real-world applications that involve long sequences, such as',
    author: 'react',
    img_url: 'https://picsum.photos/seed/picsum/200/300',
  },
  {
    title:
      'Transformer models have achieved state-of-the-art results across adiverse range of domains. However, concern over the cost of training the attention mechanism to learn complex dependencies between distant inputs continues to grow. In response, solutions that exploit the structure and sparsity of the learned attention matrix have blossomed. However, real-world applications that involve long sequences, such as',
    author: 'react',
    img_url: 'https://picsum.photos/seed/picsum/536/354',
  },
  {
    title:
      'Transformer models have achieved state-of-the-art results across adiverse range of domains. However, concern over the cost of training the attention mechanism to learn complex dependencies between distant inputs continues to grow. In response, solutions that exploit the structure and sparsity of the learned attention matrix have blossomed. However, real-world applications that involve long sequences, such as',
    author: 'react',
    img_url: 'https://picsum.photos/seed/picsum/200/300',
  },
  {
    title:
      'Transformer models have achieved state-of-the-art results across adiverse range of domains. However, concern over the cost of training the attention mechanism to learn complex dependencies between distant inputs continues to grow. In response, solutions that exploit the structure and sparsity of the learned attention matrix have blossomed. However, real-world applications that involve long sequences, such as',
    author: 'react',
    img_url: 'https://picsum.photos/seed/picsum/536/354',
  },
  {
    title:
      'Transformer models have achieved state-of-the-art results across adiverse range of domains. However, concern over the cost of training the attention mechanism to learn complex dependencies between distant inputs continues to grow. In response, solutions that exploit the structure and sparsity of the learned attention matrix have blossomed. However, real-world applications that involve long sequences, such as',
    author: 'react',
    img_url: 'https://picsum.photos/seed/picsum/200/300',
  },
  {
    title:
      'Transformer models have achieved state-of-the-art results across adiverse range of domains. However, concern over the cost of training the attention mechanism to learn complex dependencies between distant inputs continues to grow. In response, solutions that exploit the structure and sparsity of the learned attention matrix have blossomed. However, real-world applications that involve long sequences, such as',
    author: 'react',
    img_url: 'https://picsum.photos/seed/picsum/536/354',
  },
  {
    title:
      'Transformer models have achieved state-of-the-art results across adiverse range of domains. However, concern over the cost of training the attention mechanism to learn complex dependencies between distant inputs continues to grow. In response, solutions that exploit the structure and sparsity of the learned attention matrix have blossomed. However, real-world applications that involve long sequences, such as',
    author: 'react',
    img_url: 'https://picsum.photos/seed/picsum/200/300',
  },
  {
    title:
      'Transformer models have achieved state-of-the-art results across adiverse range of domains. However, concern over the cost of training the attention mechanism to learn complex dependencies between distant inputs continues to grow. In response, solutions that exploit the structure and sparsity of the learned attention matrix have blossomed. However, real-world applications that involve long sequences, such as',
    author: 'react',
    img_url: 'https://picsum.photos/seed/picsum/200/300',
  },
  {
    title:
      'Transformer models have achieved state-of-the-art results across adiverse range of domains. However, concern over the cost of training the attention mechanism to learn complex dependencies between distant inputs continues to grow. In response, solutions that exploit the structure and sparsity of the learned attention matrix have blossomed. However, real-world applications that involve long sequences, such as',
    author: 'react',
    img_url: 'https://picsum.photos/seed/picsum/200/300',
  },
];
